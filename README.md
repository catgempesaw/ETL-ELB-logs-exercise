# ETL Exercise: ELB Log Essentials to MySQL

## ğŸ“Œ Objective

This exercise demonstrates how to extract AWS Application Load Balancer (ALB) access logs from an S3 bucket, transform key log data, and load the processed results into a MySQL database table named `elb_log_data`.

---

## âš™ï¸ Technology Stack

- **Python** â€“ scripting the ETL process  
- **boto3** â€“ access and download logs from AWS S3  
- **pandas** â€“ for optional data manipulation  
- **SQLAlchemy** â€“ connect and insert into MySQL  
- **AWS S3** â€“ source of gzipped ELB access logs  
- **MySQL** â€“ target database for the transformed data

---

## ğŸ“¥ Input: ELB Access Logs

- **Source**: AWS S3 bucket (you will be provided a bucket name and prefix)
- **Format**: ALB access logs, space-delimited  
- **Compression**: May be `.gz` compressed files

## ğŸ”„ ETL Workflow

### âœ… Extraction
- Connect to S3 using `boto3`
- List and download `.gz` log files from the specified prefix
- Read and decode each line

### âœ… Transformation
For each log entry:
- Parse standard ALB fields (quoted and unquoted)
- Extract and convert:
  - `timestamp` â†’ datetime in US/Eastern timezone
  - `client_ip` â†’ from `client_ip:port`
  - `http_method` and `requested_path` â†’ from the `"request"` field
  - `user_agent_full` â†’ store entire UA string
  - `ua_browser_family`, `ua_os_family` â†’ parsed using `httpagentparser`
  - `total_processing_time_ms` â†’ sum of 3 timing fields (in ms)
  - `received_bytes` and `sent_bytes` â†’ integers
  - `log_source_file` â†’ S3 object key

### âœ… Loading
- Insert transformed rows into the `elb_log_data` table in MySQL using SQLAlchemy
